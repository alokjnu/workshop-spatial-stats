[["index.html", "Introduction to spatial statistics in R Overview", " Introduction to spatial statistics in R Wesley Brooks 2021-05-04 Overview The key insight about spatial statistics is that measurements from nearby locations are more alike than measurements from distant locations. Spatial statistics is a vast subfield with relatively few practitioners. So there has been less development of uniform naming and practices, which can result in a wild-west atmosphere. Let’s take a look at the CRAN “Spatial” task view, which will give us a sense of the confusion that reigns in this subject: https://CRAN.R-project.org/view=Spatial. One workshop cannot possibly teach you how to “do” spatial statistics, so the goal here today is to learn the basics and help to limit the information overload that might come from just cracking open the fire hose. When it comes to analyzing spatial data in R, you will hopefully learn where to look first and what R packages to reach for. "],["spatial-data.html", "1 Spatial data", " 1 Spatial data There are three basic kinds of spatial data that I’ll try to introduce today: Geostatistical data: measurements of a continuous phenomenon, which is observed at some set of points. Rongelap Island radioactivity. gstat package. Areal data: each observation corresponds to an area, like a county or a raster cell. Point data: arises from watching an area to see where and how often some event will occur. Geostatistical and areal data are more closely related to each other than point data because both of them deal with values that you go to a particular place and measure. "],["random-fields.html", "2 Random fields", " 2 Random fields “[The Force] surrounds and penetrates us. It binds the galaxy together.” - Obi-Wan Kenobi library( &quot;RandomFields&quot; ) A random field is like The Force - it is everywhere without being seen. Air particulate concentration, ground water level, soil lead concentration - you may be aware that these phenomena are everywhere. Sociological phenomena like public sentiment or land value may also be thought of as an ether or field that flows everywhere. But our data doesn’t surround and penetrate us; it doesn’t flow through everything. Data, the observations that we actually have to work from, are quite sparse measurements of the underlying field. So our task in spatial data analysis (what makes it different from other kinds of data analysis) is generally to estimate parameters of a random field. Here I’ve prepared a couple of graphics to try to connect spatial statistics to an intuition about spatial data. The reason that we always use the normal distribution for a spatial field is because it is the foremost among VERY few distributions that have a multivariate form. rongelap-illustration rongelap-illustration # set range parameter range = 2 # make a plot of the exponential correlation tt = seq(0, 10, length.out=300) cov_fun = exp( -tt / range) plot( tt, cov_fun, type=&#39;l&#39;, bty=&#39;n&#39;, xlab=&quot;distance&quot;, ylab=&#39;correlation&#39;) # same plot, but as a variogram nugget = 0.2 plot( tt, 1 - cov_fun + nugget, type=&#39;l&#39;, bty=&#39;n&#39;, ylim = c(0, 1.3), xlab=&quot;distance&quot;, ylab=&#39;semivariance&#39;) To illustrate observations from a random field, let’s do a bit of simulation. I’ll use the RandomFields package, so you’ll need to install that. The RMexp() function uses the exponential covariance function. I’m creating four independent realizations of data so that you can see what the variability looks like. # simulate six realizations with an exponential model model &lt;- RMexp() x &lt;- seq(0, 10, 0.1) z &lt;- RFsimulate(model, x, x, n=4) ## .... ## New output format of RFsimulate: S4 object of class &#39;RFsp&#39;; ## for a bare, but faster array format use &#39;RFoptions(spConform=FALSE)&#39;. # plot the simulations plot( z ) Out of curiosity, let’s see how those would look if we used another covariance function. Here I’m using cubic covariance, which is important only insofar as you can see that any effect on the data isn’t obvious. # The effect of the covariance model is typically not obvious: model &lt;- RMcubic() z &lt;- RFsimulate(model, x, x, n=4) ## .... plot( z ) The fact that the proper covariance function is seldom obvious means that the ultimate choice often seems fairly arbitrary. This contributes to why spatial statistics is so balkanized, with dozens of very closely related methods that use incompatible language and software. "],["geostatistics.html", "3 Geostatistics 3.1 Example: Rongelap radiation 3.2 Example: Meuse River zinc pollution", " 3 Geostatistics library( &quot;gstat&quot; ) library( &quot;stars&quot; ) library( &quot;readr&quot; ) library( &quot;ggplot2&quot; ) 3.1 Example: Rongelap radiation To illustrate basic concepts of geostatistics, I will use measurements of gamma rays on Rongelap atoll, which was irradiated by fallout in 1954 due to the Castle Bravo nuclear test at Bikini atoll, which was three times as powerful as planned. Let’s load the data. # load the point data rongelap = read_csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-spatial-stats/master/data/rongelap.csv&quot;) ) rongelap = st_as_sf( rongelap, coords = c(&#39;x&#39;, &#39;y&#39;) ) # load the outline of the island load( url(&quot;https://github.com/ucdavisdatalab/workshop-spatial-stats/blob/master/data/rongelap-outline.rdata?raw=true&quot;) ) Let’s check the histogram, which suggests that a log transformation will help stabilize the variance. Then plot the data over the shape of Rongelap (you’ve seen this already). # check the histogram to see about a log transform with( rongelap, hist(val) ) with( rongelap, hist( log(val) )) # plot the data on a two-way gradient to exaggerate differences ggplot(rongelap_shp) + geom_sf() + geom_sf(data=rongelap, mapping=aes(color=log(val))) + scale_color_gradient2(limits=c(4, 10), midpoint=7.5, low=&quot;blue&quot;, high=&quot;red&quot;) 3.1.1 Kriging The fundamental method of geostatistical analysis is kriging (named for a South African mining engineer who invented it). To do kriging, we predict the gamma rays at unobserved locations based on a model of the field’s mean and variance functions. To begin with, I’ll assume the mean is constant and the variance function is exponential (which is the model we saw before). The predictions are made at held-out locations so that we can compare them to the actual, observed values. # hold out some observations from the rongelap data set indx = sample( 1:nrow(rongelap), 30 ) r2 = rongelap[ -indx, ] r3 = rongelap[ indx, ] # make a kriging model rongelap_fit = krige(log(val) ~ 1, r2, r3, model=vgm(0.5, &quot;Exp&quot;, 500, 0.05)) ## [using ordinary kriging] # plot the result ggplot(rongelap_shp) + geom_sf() + geom_sf(data=rongelap_fit, mapping=aes(color=var1.pred)) + scale_color_gradient2(limits=c(4, 10), midpoint=7.5, low=&quot;blue&quot;, high=&quot;red&quot;) # compare the fitted to the actual plot( rongelap_fit$var1.pred, log(r3$val), bty=&#39;n&#39; ) abline(0, 1) 3.2 Example: Meuse River zinc pollution OK, now another geostatistical data set. This time, the data are measurements of pollution in digs within a bend of the Meuse River in northern France. We will focus on zinc pollution. First thing is to load and transform the data: # load the meuse data meuse = read_csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-spatial-stats/master/data/meuse.csv&quot;) ) meuse = st_as_sf( meuse, coords=c( &#39;x&#39;, &#39;y&#39;) ) # look at the zinc concentration for a possible log transform with( meuse, hist(zinc) ) with( meuse, hist( log(zinc) )) Now let’s plot the zinc concentration. It looks like the most intense pollution is along the river itself. # bubble plot of the Meuse River zinc data ggplot(meuse) + geom_sf( mapping = aes(size=zinc), color=&quot;green&quot;, alpha=0.6) 3.2.1 Kriging the Meuse data Once again, we will use kriging to predict the zinc pollution in locations where no soil samples were taken. Recall that kriging estimates the random field via the mean and covariance functions. This time, rather than providing a pre-specified covariance function, I am going to estimate it. And I will estimate the mean as not constant, but as a function of distance from the river. # make a variogram of the meuse data vario = variogram( log(zinc) ~ sqrt(dist), data=meuse ) vario_fit = fit.variogram( vario, model = vgm(1, &quot;Exp&quot;, 900, 1)) # show the fitted variogram and its parameters: vario_fit ## model psill range ## 1 Nug 0.05712112 0.0000 ## 2 Exp 0.17641525 340.3105 plot( vario, vario_fit ) That’s our estimated covariance function. Now let’s make predictions at the locations specified in meuse.grid and plot them. # load the prediction locations meuse.grid = read_csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-spatial-stats/master/data/meuse.grid.csv&quot;) ) meuse.grid = st_as_sf( meuse.grid, coords = c(&#39;x&#39;, &#39;y&#39;) ) # regression kriging for the log zinc concentration: meuse_fit = krige( log(zinc) ~ sqrt(dist), meuse, meuse.grid, model=vario_fit) ggplot( meuse_fit ) + geom_sf( mapping = aes( color=var1.pred )) + scale_color_gradient(low=grey(0.7), high=&quot;red&quot;) + theme_bw() # plot the uncertainty of the kriging estimator ggplot( meuse_fit ) + geom_sf( mapping = aes( color=var1.var )) + scale_color_gradient(low=grey(0.7), high=&quot;red&quot;) + theme_bw() Note that the estimated concentration is greatest near the river, and the uncertainty of the predictions grows as you get farther away from measured data. "],["areal-data.html", "4 Areal data 4.1 Example: North Carolina SIDS 4.2 Neighbor weighting 4.3 Moran’s I", " 4 Areal data library( &quot;spdep&quot; ) library( &quot;tigris&quot; ) library( &quot;sf&quot; ) library( &quot;ggplot2&quot; ) library( &quot;readr&quot; ) 4.1 Example: North Carolina SIDS To illustrate some basics of areal data, we’ll use data on the rate of sudden infant death in 1979 North Carolina. Here we import the data and the shapes of the counties. # import the north carolina county shapefiles cty = counties() nc_cty = cty[ cty$STATEFP == &quot;37&quot;, ] nc_cty = st_simplify( nc_cty, dTolerance=0.01 ) # import the north carolina SIDS data nc_sids = read_csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-spatial-stats/master/data/nc_sids.csv&quot;) ) nc_sids = st_as_sf( nc_sids, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = &quot;epsg:4269&quot; ) Match the data to the shapes and plot the rate of SIDS: # join the shapes and data indx = match( nc_cty$NAME, nc_sids$name ) nc_cty = cbind( nc_cty, nc_sids[indx, ] ) # calculate rate of SIDS in 1979 nc_cty = within(nc_cty, sids_rate &lt;- SID79 / BIR79) # plot the data ggplot(nc_cty) + geom_sf(mapping = aes(fill=sids_rate)) + scale_fill_gradient( low=grey(0.5), high=&#39;red&#39; ) + theme_bw() 4.2 Neighbor weighting While geostatistical covariance is based on the distance from points, we can’t calcuate those distances for areal data because there is no unique distance between areas. For example, how far you are from Sacramento county depends on where in Yolo county you stand. So for areal data, the covariance is based on whether areas share a border. This has the added benefit of making for much faster calculations (because most areas do not touch each other). Here we’ll create a neighborhood for the North Carolina counties: # make the neighborhood nc_nb = poly2nb( nc_cty, queen=TRUE, row.names=nc_cty$NAME ) # plot the neighborhod plot( st_geometry(nc_cty) ) plot( nc_nb, coords = st_centroid( st_geometry(nc_cty), of_largest_polygon ), add = TRUE, col=&quot;blue&quot;) ## Warning in st_centroid.sfc(st_geometry(nc_cty), of_largest_polygon): st_centroid ## does not give correct centroids for longitude/latitude data 4.3 Moran’s I The basic test for the presence of autocorrelation between neighbors is called Moran’s I and it is in the spdep package. # calculate neighbor weights (type W) W = nb2listw(nc_nb) moran.test( nc_cty$sids_rate, W ) ## ## Moran I test under randomisation ## ## data: nc_cty$sids_rate ## weights: W ## ## Moran I statistic standard deviate = 1.5529, p-value = 0.06022 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.098926833 -0.010101010 0.004929027 Here, the result of moran.test is a p-value of 0.06, so the SIDS data are consistent with the absence of a spatial trend. When Moran’s I is positive the data appear clustered, and when I is negative, the data appear dispersed. "],["point-processes.html", "5 Point processes 5.1 Import and plot valley oak locations 5.2 Poisson point process 5.3 Ripley’s K", " 5 Point processes library( &quot;spatstat&quot; ) library( &quot;sf&quot; ) library( &quot;tigris&quot; ) library( &quot;dplyr&quot; ) library( &quot;readr&quot; ) library( &quot;ggplot2&quot; ) 5.1 Import and plot valley oak locations To introduce point patterns, I will use data from iNaturalist on the locations of valley oaks in Sacramento County. First, we will import the data and the Sac county shapefile. oak = read_csv( url(&quot;https://raw.githubusercontent.com/ucdavisdatalab/workshop-spatial-stats/master/data/oak.csv&quot;) ) # convert oak to an sf object oak = st_as_sf( oak, coords = c(&quot;longitude&quot;, &quot;latitude&quot; ), crs = &quot;epsg:4326&quot; ) # project oaks to California Albers oak = st_transform( oak, crs = &quot;epsg:6414&quot; ) Now, let’s plot the points. # load sac county shape cty = counties() saccty = filter( cty, NAME == &quot;Sacramento&quot; ) saccty = st_transform( saccty, crs = &quot;epsg:6414&quot; ) # plot the oak locations in Sac county ggplot(saccty) + geom_sf() + geom_sf(data=oak) + theme_bw() 5.2 Poisson point process The Poisson point process is a very simple point process that assumes that if you draw any square on the region, then the number of points inside the square should follow a Poisson distribution, with the expected number of points only depending on the area of the square. Based on the clustering of valley oaks (mostly around the American River), it looks like the number of trees would depend on the location of the square, not just its size. That would imply that this is not a simple Poisson point process. Let’s back up that eye test by looking at Ripley’s K function - a measure of how clustered or dispersed the points are. Either clustering or disepersion would be a violation of the Poisson assumptions. Begin by converting the point locations into a ppp object (spatstat package). # get the locations of the oaks and the bounds of the county oak_locs = st_coordinates( oak ) # turn the oak locations into a Poisson point pattern: oak_pts = ppp( oak_locs[, 1], oak_locs[, 2], window = as.owin( saccty ) ) ## Warning: data contain duplicated points 5.3 Ripley’s K Now calculate the Ripley’s K function, and just for fun I’ll include an estimated density showing hotspots (this density estimate is over-simple but still suggestive.) # plot the point pattern: plot( oak_pts ) # check whether the points follow a Poisson distribution (no) plot( Kest( oak_pts ) ) plot( envelope( oak_pts, Kest)) ## Generating 99 simulations of CSR ... ## 1, 2, [etd 7:46] 3, [etd 6:45] 4, ## [etd 6:38] 5, [etd 6:50] 6, [etd 6:38] 7, [etd 6:42] 8, ## [etd 6:37] 9, [etd 6:20] 10, [etd 6:21] 11, [etd 7:13] 12, ## [etd 7:00] 13, [etd 6:46] 14, [etd 6:35] 15, [etd 6:23] 16, ## [etd 6:14] 17, [etd 6:05] 18, [etd 5:58] 19, [etd 5:50] 20, ## [etd 5:48] 21, [etd 5:45] 22, [etd 5:38] 23, [etd 5:28] 24, ## [etd 5:25] 25, [etd 5:24] 26, [etd 5:20] 27, [etd 5:14] 28, ## [etd 5:08] 29, [etd 5:00] 30, [etd 4:53] 31, [etd 4:48] 32, ## [etd 4:44] 33, [etd 4:39] 34, [etd 4:35] 35, [etd 4:28] 36, ## [etd 4:23] 37, [etd 4:18] 38, [etd 4:13] 39, [etd 4:09] 40, ## [etd 4:06] 41, [etd 4:02] 42, [etd 3:58] 43, [etd 3:53] 44, ## [etd 3:49] 45, [etd 3:44] 46, [etd 3:40] 47, [etd 3:35] 48, ## [etd 3:31] 49, [etd 3:26] 50, [etd 3:22] 51, [etd 3:18] 52, ## [etd 3:13] 53, [etd 3:09] 54, [etd 3:05] 55, [etd 3:01] 56, ## [etd 2:57] 57, [etd 2:52] 58, [etd 2:47] 59, [etd 2:44] 60, ## [etd 2:39] 61, [etd 2:35] 62, [etd 2:31] 63, [etd 2:27] 64, ## [etd 2:22] 65, [etd 2:18] 66, [etd 2:14] 67, [etd 2:10] 68, ## [etd 2:06] 69, [etd 2:02] 70, [etd 1:57] 71, [etd 1:53] 72, ## [etd 1:49] 73, [etd 1:45] 74, [etd 1:40] 75, [etd 1:36] 76, ## [etd 1:32] 77, [etd 1:28] 78, [etd 1:24] 79, [etd 1:20] 80, ## [etd 1:16] 81, [etd 1:12] 82, [etd 1:08] 83, [etd 1:04] 84, ## [etd 1:00] 85, [etd 55 sec] 86, [etd 51 sec] 87, [etd 47 sec] 88, ## [etd 43 sec] 89, [etd 39 sec] 90, [etd 35 sec] 91, [etd 32 sec] 92, ## [etd 28 sec] 93, [etd 24 sec] 94, [etd 20 sec] 95, [etd 16 sec] 96, ## [etd 12 sec] 97, [etd 8 sec] 98, [etd 4 sec] 99. ## ## Done. # plot the estimated isotropic density: plot( density( oak_pts ) ) The observed K-function lies entirely above the plausible envelope that would imply a simple Poisson point process. Above the theoretical K means the points are clustered (below would mean dispersed). "]]
